{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "x2t29JtV7hC-",
        "uJ0Sizzg7nZq",
        "b6P5tVS_ry_z",
        "Pw0fMgZXs-sM",
        "ruhUCfwjHRQD",
        "FEemx-pl6_kX",
        "iGWzgZ3QYfFD",
        "lvjcgom2qQLw",
        "MUknx-olqcQV",
        "NOmkHD2Yqogi",
        "ScUgqJF9rN5x"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scubadriver/gnbook/blob/main/Copy_of_Multi_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "x2t29JtV7hC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code simply imports the required libraries into your environment. Key here are torch, which is pytorch, and torchvision, which is an additional library full of models, examples, datasets and so on."
      ],
      "metadata": {
        "id": "Me5mH4zKKPKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY_SZAKRq1KT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.models import resnet18"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PlantNet Dataset"
      ],
      "metadata": {
        "id": "uJ0Sizzg7nZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [plantnet](https://plantnet.org/en/2021/03/30/a-plntnet-dataset-for-machine-learning-researchers/) dataset is a popular dataset for those working with biological images. It's huge, with I think over 1000 classes. This makes for quite an interesting dataset, but here we are only using a small subset. I took some of the best represented (most images) species, and chose only 32 of these. For these I have access to the species and also whether or not the image is of flowers or not.\n",
        "\n",
        "If you'd like to learn to do AI, I recommend starting with datasets like this, MNIST, CIFAR10 etc to give you an introduction before moving on to more challenging datasets.\n",
        "\n",
        "**Datasets**\n",
        "\n",
        "In pytorch a dataset is represented here by a VisionDataset class. There are built in datasets available for use, but here I've written my own by adapting the code from the pytorch repo. This dataset checks for a local copy of the plantnet mini dataset, and downloads it if it doesn't find it. It then exposes a couple of key methods, `__len__()` returns the total number of images in the dataset, and `__getitem__(i)` lets you retreive a specific image and label. This allows you to use indices such as `dataset[145]` to get image+labels 145."
      ],
      "metadata": {
        "id": "KnrA4cRBKSM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets.utils import download_and_extract_archive, extract_archive, verify_str_arg, check_integrity\n",
        "from torchvision.datasets.vision import VisionDataset\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple\n",
        "\n",
        "class PlantNetMini(VisionDataset):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        train: bool = True,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None\n",
        "    ) -> None:\n",
        "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
        "        self.train = train  # training set or test set\n",
        "\n",
        "        self.download()\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError(\"Dataset not found.\")\n",
        "\n",
        "        self.species, self.annotations = self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        data = torch.load(os.path.join(self.data_folder, \"annotations.pth\"))\n",
        "        annotations = data[\"annotations\"][\"train\"] if self.train else data[\"annotations\"][\"valid\"]\n",
        "        return data[\"species_names\"], annotations\n",
        "\n",
        "    def __getitem__(self, index: int) -> Tuple[Any, Any, Any]:\n",
        "        dt = self.annotations[index]\n",
        "\n",
        "        img_path = os.path.join(self.data_folder, dt[\"file_name\"])\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        label = dt[\"class_id\"]\n",
        "        is_flower = int(dt[\"flower\"]) # 0 or 1\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, label, is_flower\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def get_raw_item(self, index):\n",
        "        dt = self.annotations[index]\n",
        "\n",
        "        img_path = os.path.join(self.data_folder, dt[\"file_name\"])\n",
        "        img = Image.open(img_path)\n",
        "\n",
        "        label = dt[\"class_id\"]\n",
        "        is_flower = dt[\"flower\"]\n",
        "\n",
        "        return img, label, is_flower\n",
        "\n",
        "    @property\n",
        "    def data_folder(self) -> str:\n",
        "        return os.path.join(self.root, self.__class__.__name__)\n",
        "\n",
        "    def _check_exists(self) -> bool:\n",
        "        annotation_file_path = os.path.join(self.data_folder, \"annotations.pth\")\n",
        "        return os.path.isfile(annotation_file_path)\n",
        "\n",
        "    def download(self) -> None:\n",
        "        if self._check_exists():\n",
        "            return\n",
        "\n",
        "        os.makedirs(self.data_folder, exist_ok=True)\n",
        "\n",
        "        zip_url = \"https://drive.google.com/open?id=10m7E5qrEPdqog-0j6HFKiGAS0SsejUuJ&authuser=1\"\n",
        "        zip_name = \"plantnet-mini.zip\"\n",
        "\n",
        "        # download files\n",
        "        try:\n",
        "            download_and_extract_archive(zip_url, download_root=self.data_folder, filename=zip_name, md5=None)\n",
        "        except URLError as error:\n",
        "            print(f\"Failed to download:\\n{error}\")\n",
        "        finally:\n",
        "            print()"
      ],
      "metadata": {
        "id": "YxVn1yy4rycP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's very important to split your data into separate training and validation/testing sets. I already included a split in the zip download, so if we pass a train=True or train=False variable to the dataset constructor, it will choose the correct images.\n",
        "\n",
        "When we create the datasets, we also add code to inform the dataset how to provide random augmentations to images during training. Data augmentation (such as randomply flipping the image) can be useful to add variety to your training data.\n",
        "\n",
        "The output of both the training and validation sets are images that are 224x224 pixels."
      ],
      "metadata": {
        "id": "g5k3kv3aKUTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "train_set = PlantNetMini('../data', train=True, transform=transforms.Compose([\n",
        "            transforms.Resize(384),\n",
        "            transforms.RandomResizedCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize]))\n",
        "\n",
        "valid_set = PlantNetMini('../data', train=False, transform=transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize]))"
      ],
      "metadata": {
        "id": "Nia0BApr37n4",
        "outputId": "9237710e-8bd6-4de4-d00c-924908f0e227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ../data/PlantNetMini/plantnet-mini.zip\n",
            "Extracting ../data/PlantNetMini/plantnet-mini.zip to ../data/PlantNetMini\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'URLError' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1bdb7340c451>\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Extracting {archive} to {extract_root}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mextract_archive\u001b[0;34m(from_path, to_path, remove_finished)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mextractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mremove_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_extract_zip\u001b[0;34m(from_path, to_path, compression)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_extract_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m     with zipfile.ZipFile(\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mfrom_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_ZIP_COMPRESSION_MAP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZIP_STORED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1335\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3e6fbc185951>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m train_set = PlantNetMini('../data', train=True, transform=transforms.Compose([\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomResizedCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1bdb7340c451>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m  \u001b[0;31m# training set or test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-1bdb7340c451>\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzip_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to download:\\n{error}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'URLError' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preview Random Training Image**\n",
        "\n",
        "This small piece of code simply obtains a random image from the training set so you can see it. Alongside each image we have a number that represents its class (which we can convert to species by indexing into the species list) and a boolean representing if the image contains flowers or not."
      ],
      "metadata": {
        "id": "CiiqTdAE_w9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0,len(train_set) - 1)\n",
        "\n",
        "#img, label, is_flower = train_set.get_raw_item(idx)\n",
        "#species = train_set.species[label]\n",
        "#img = img.resize((360,360))\n",
        "#print (f\"Class: {label}, {species} Is Flower: {is_flower}\")\n",
        "#display(img)\n",
        "\n",
        "img, label, is_flower = train_set[idx]\n"
      ],
      "metadata": {
        "id": "ighhh-MQOiBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading"
      ],
      "metadata": {
        "id": "b6P5tVS_ry_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch works by sampling from a dataset to train a network. The actual class that does this sampling is usually a DataLoader. Often you don't need to code this yourself, you simply pass various parameters along with the dataset and everything else is handled for you. HEre we set the batch size (number of images we sample and train on at once) alongside other parametes such as 2 workers for loading and preparing the images."
      ],
      "metadata": {
        "id": "4l3nbdv2tOM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "test_batch_size = 32\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_kwargs = {'batch_size': batch_size}\n",
        "test_kwargs = {'batch_size': test_batch_size}\n",
        "\n",
        "cuda_kwargs = {'num_workers': 2,\n",
        "               'pin_memory': True,\n",
        "               'shuffle': True}\n",
        "train_kwargs.update(cuda_kwargs)\n",
        "test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(valid_set, **test_kwargs)"
      ],
      "metadata": {
        "id": "8vyi5Ko2RxGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Pw0fMgZXs-sM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model in pytorch is our actual neural network. Here we're using a convolutiona neural network (CNN) called a ResNet. This is a very popular network, the [paper](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has 140k citations! The exact reason resnets are popular we won't cover here, but the main thing to understand is we are taking batches of RGB images in the form BxCxHxW e.g. 32x3x224x224, and converting these into predictions of class for each of the 32 images, i.e. 32x32, which is for each image, a list of 32 values representing the likelihood of any particular class. To make this work we have to take our resnet, and alter the default 1000 classes in the last layer to be only 32. In this code model.fc is the last layer."
      ],
      "metadata": {
        "id": "cIDoxMoKtpC0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cuda = True\n",
        "num_classes = 32"
      ],
      "metadata": {
        "id": "H5oeAygBsBr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(pretrained=True)\n",
        "model.num_classes=32\n",
        "channels_in = model.fc.in_features\n",
        "model.fc = nn.Linear(channels_in, 32)\n",
        "\n",
        "model = model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "RsA3qbfC69P7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arr = torch.randn((10,3,224,224))\n",
        "arr = arr.to(\"cuda\")\n",
        "output = model(arr)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "_ilHxsS33AN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "ruhUCfwjHRQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you print the model, torch will show you a summary of all of the layers."
      ],
      "metadata": {
        "id": "44p8LLO3ubq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "Q-vxsXfGHPNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "FEemx-pl6_kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to train the network! If you run this cell, you're actually doing deep learning! We define two functions here, `train` and `test`. Train runs one epoch of training: we sample batches of 32 images and run these through the network until we have used every image in the dataset once. We can then perform more epochs as desired. In this code we are ignoring the flower label, and only predicting class. The key lines are:\n",
        "\n",
        "`for batch_idx, (data, target, flower) in enumerate(train_loader):`\n",
        "\n",
        "This iterates over batches in the dataset, with the dataloader handling all the multithreading and preparation for us.\n",
        "\n",
        "`optimizer.zero_grad()`\n",
        "\n",
        "This sets the gradients (the directions to move the weights to improve the performance of the network) to zero ready to train on this batch.\n",
        "\n",
        "`output = model(data)`\n",
        "\n",
        "`loss = F.cross_entropy(output, target)`\n",
        "\n",
        "These lines put the batch of images through the model to get a prediction, and then calculate our error / loss by comparing the output to the actual species labels for these 32 images. Intuitively, if the estimate of the network is better, the loss will be lower.\n",
        "\n",
        "`loss.backward()`\n",
        "\n",
        "`optimizer.step()`\n",
        "\n",
        "Finally, we use the loss to calculate new gradients by calling `backward()`. This runs an algorithm called back propagation, which put simply calculates in which direction we could move each weight in the network to make the loss a little smaller next time. We then actually move these weights a little using the `step` function."
      ],
      "metadata": {
        "id": "W5xPvtNkukAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target, flower) in enumerate(train_loader):\n",
        "        data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "uxXpxzgNsFhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testing code is very similar, except we're running on the testing set, and calculating a percentage accuracy. You don't train on the test set, so we aren't using any loss or optimiser code."
      ],
      "metadata": {
        "id": "d4km2gHEv_3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for (data, target, flower) in test_loader:\n",
        "            data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "            output = model(data)\n",
        "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "Cx2m7f__sMTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "gamma = 0.7\n",
        "log_interval = 10\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train(model, train_loader, optimizer, epoch)\n",
        "        test(model, test_loader)\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "ZfYk1O5et64p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Task"
      ],
      "metadata": {
        "id": "iGWzgZ3QYfFD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For those of you following up to here, well done! This last bit of code is just a neat extra to give you an idea of some of the extra things you can do with CNNs. Here I am extending our previous design to include two outputs, a 32 class species classification as above, and also a 2 class yes/no flower classification. By using multiple loss functions, we can actually train the network to solve both problems at the same time. Using multiple losses (multi-task learning) is fairly common, as it often lets us provide additional information to a network during training, improving overall performance."
      ],
      "metadata": {
        "id": "MEunaHvlwOXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-task FC module"
      ],
      "metadata": {
        "id": "lvjcgom2qQLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskHead(nn.Module):\n",
        "    def __init__(self, channels_in, num_classes):\n",
        "        super(MultiTaskHead, self).__init__()\n",
        "\n",
        "        self.heads = []\n",
        "        for idx, current_num_classes in enumerate(num_classes):\n",
        "            s = nn.Sequential(\n",
        "                nn.Linear(channels_in, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, current_num_classes)\n",
        "            )\n",
        "            self.add_module(f\"Head {idx}\", s)\n",
        "            self.heads.append(s)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = []\n",
        "        for head in self.heads:\n",
        "            output.append(head.forward(x))\n",
        "\n",
        "        return tuple(output)"
      ],
      "metadata": {
        "id": "5_hhQ70yYjLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create multi-task head and insert onto the end of Resnet18"
      ],
      "metadata": {
        "id": "MUknx-olqcQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18(pretrained=True)\n",
        "model.num_classes = 32\n",
        "channels_in = model.fc.in_features\n",
        "\n",
        "head = MultiTaskHead(channels_in,[32,2])\n",
        "model.fc = head\n",
        "\n",
        "model = model.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "WMQtgNdEcWs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extend training and validation code"
      ],
      "metadata": {
        "id": "NOmkHD2Yqogi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target, flower) in enumerate(train_loader):\n",
        "        data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "        flower = flower.to(\"cuda\")\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "\n",
        "        class_output = output[0]\n",
        "        flower_output = output[1]\n",
        "\n",
        "        loss = F.cross_entropy(class_output, target)\n",
        "        loss2 = F.cross_entropy(flower_output, flower)\n",
        "\n",
        "        final_loss = loss + 0.1 * loss2\n",
        "        final_loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLoss2: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item(), loss2.item()))"
      ],
      "metadata": {
        "id": "0NoXrffNq7D6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    flower_loss = 0\n",
        "    correct = 0\n",
        "    correct_flower = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for (data, target, flower) in test_loader:\n",
        "            data, target = data.to(\"cuda\"), target.to(\"cuda\")\n",
        "            flower = flower.to(\"cuda\")\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            class_output = output[0]\n",
        "            flower_output = output[1]\n",
        "\n",
        "            test_loss += F.cross_entropy(class_output, target, reduction='sum').item()  # sum up batch loss\n",
        "            flower_loss += F.cross_entropy(flower_output, flower, reduction='sum').item()\n",
        "\n",
        "            pred = class_output.argmax(dim=1, keepdim=True)\n",
        "            pred2 = flower_output.argmax(dim=1, keepdim=True)\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            correct_flower += pred2.eq(flower.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%), Flower Acc  {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset),\n",
        "        correct_flower, len(test_loader.dataset), 100. * correct_flower / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "id": "YgsXhpXBq_Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retrain multi-task"
      ],
      "metadata": {
        "id": "ScUgqJF9rN5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 6\n",
        "learning_rate = 0.01\n",
        "gamma = 0.7\n",
        "log_interval = 10\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train(model, train_loader, optimizer, epoch)\n",
        "        test(model, test_loader)\n",
        "        scheduler.step()"
      ],
      "metadata": {
        "id": "vD7WI3Nicd4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randint(0,len(train_set) - 1)\n",
        "\n",
        "img, label, is_flower = train_set.get_raw_item(idx)\n",
        "species = train_set.species[label]\n",
        "img = img.resize((360,360))\n",
        "print (f\"Class: {label}, {species} Is Flower: {is_flower}\")\n",
        "display(img)\n",
        "\n",
        "data, label, flower = train_set[idx]\n",
        "data = data.unsqueeze(0)\n",
        "data = data.to(\"cuda\")\n",
        "#flower = flower.to(\"cuda\")\n",
        "\n",
        "output = model(data)\n",
        "\n",
        "class_output = output[0]\n",
        "flower_output = output[1]\n",
        "\n",
        "print (class_output)\n",
        "pred = class_output.argmax(dim=1, keepdim=True)\n",
        "print (pred)\n"
      ],
      "metadata": {
        "id": "-bJ83ajU9LVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flower_output"
      ],
      "metadata": {
        "id": "8udeDlGP9Zlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KgTg4dIp-s2P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}